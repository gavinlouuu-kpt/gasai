{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importance_calculator import process_experiments, setup_directory, spec_exp_imp\n",
    "# load data from catalog\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from captum.attr import IntegratedGradients\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "def sequence_generate(sequence_length: int, data_set: pd.DataFrame, features: list, target_column: str, step_size: int):\n",
    "    # Initialize lists to hold sequences and targets\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    # Group by 'exp_no' and create sequences for each group\n",
    "    for _, group in data_set.groupby('exp_no'):\n",
    "        data = group[features].values\n",
    "        target_data = group[target_column].values\n",
    "        # Create sequences with specified step size\n",
    "        for i in range(0, len(group) - sequence_length, step_size):\n",
    "            # Extract the sequence of features and the corresponding target\n",
    "            sequence = data[i:(i + sequence_length)]\n",
    "            target = target_data[i + sequence_length]  # Target is the next record\n",
    "            sequences.append(sequence)\n",
    "            targets.append(target)\n",
    "    # Convert lists to numpy arrays\n",
    "    sequences_np = np.array(sequences)\n",
    "    targets_np = np.array(targets)\n",
    "    return sequences_np, targets_np\n",
    "\n",
    "def split_data(sequences, targets, test_size=0.2, random_state=42):\n",
    "    sequences_train, sequences_test, targets_train, targets_test = train_test_split(\n",
    "        sequences, targets, test_size=test_size, random_state=random_state)\n",
    "    return sequences_train, sequences_test, targets_train, targets_test\n",
    "\n",
    "\n",
    "def scale_sequences(sequences_train, sequences_test):\n",
    "    scaler = StandardScaler()\n",
    "    n_samples_train, sequence_length, n_features = sequences_train.shape\n",
    "    sequences_train_reshaped = sequences_train.reshape(-1, n_features)\n",
    "    scaler.fit(sequences_train_reshaped)\n",
    "    sequences_train_scaled = scaler.transform(sequences_train_reshaped).reshape(n_samples_train, sequence_length, n_features)\n",
    "    \n",
    "    n_samples_test, _, _ = sequences_test.shape\n",
    "    sequences_test_reshaped = sequences_test.reshape(-1, n_features)\n",
    "    sequences_test_scaled = scaler.transform(sequences_test_reshaped).reshape(n_samples_test, sequence_length, n_features)\n",
    "    \n",
    "    return sequences_train_scaled, sequences_test_scaled\n",
    "\n",
    "\n",
    "def convert_to_tensors(sequences_train, sequences_test, targets_train, targets_test):\n",
    "    train_sequences_tensor = torch.tensor(sequences_train, dtype=torch.float32)\n",
    "    test_sequences_tensor = torch.tensor(sequences_test, dtype=torch.float32)\n",
    "    train_targets_tensor = torch.tensor(targets_train, dtype=torch.float32)\n",
    "    test_targets_tensor = torch.tensor(targets_test, dtype=torch.float32)\n",
    "    return train_sequences_tensor, test_sequences_tensor, train_targets_tensor, test_targets_tensor\n",
    "\n",
    "\n",
    "def create_dataloaders(train_sequences_tensor, train_targets_tensor, test_sequences_tensor, test_targets_tensor, batch_size=64):\n",
    "    train_dataset = TensorDataset(train_sequences_tensor, train_targets_tensor)\n",
    "    test_dataset = TensorDataset(test_sequences_tensor, test_targets_tensor)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "# Fully connected neural network with one hidden layer\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Set initial hidden states (and cell states for LSTM)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        out, _ = self.lstm(x, (h0,c0))  \n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "def ensure_dir(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "def setup_training(model, learning_rate, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    return criterion, optimizer\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    for sequences_batch, targets_batch in train_loader:\n",
    "        sequences_batch = sequences_batch.to(device)\n",
    "        targets_batch = targets_batch.to(device).unsqueeze(-1)\n",
    "        \n",
    "        outputs = model(sequences_batch)\n",
    "        loss = criterion(outputs, targets_batch)\n",
    "        epoch_losses.append(loss.item())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    avg_training_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "    return avg_training_loss\n",
    "\n",
    "def validate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for sequences_batch, targets_batch in test_loader:\n",
    "            sequences_batch = sequences_batch.to(device)\n",
    "            targets_batch = targets_batch.to(device).unsqueeze(-1)\n",
    "            \n",
    "            outputs = model(sequences_batch)\n",
    "            loss = criterion(outputs, targets_batch)\n",
    "            \n",
    "            total_val_loss += loss.item()\n",
    "            count += 1\n",
    "    \n",
    "    avg_val_loss = total_val_loss / count\n",
    "    return avg_val_loss\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, avg_training_loss, val_rmse, session_checkpoint_dir):\n",
    "    checkpoint_path = os.path.join(session_checkpoint_dir, f'checkpoint_epoch_{epoch+1}.pt')\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'training_loss': avg_training_loss,\n",
    "        'validation_rmse': val_rmse,\n",
    "    }, checkpoint_path)\n",
    "\n",
    "def plot_metrics(training_losses, validation_rmses, epoch, session_checkpoint_dir):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, epoch + 2), training_losses, label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Over Time')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, epoch + 2), validation_rmses, label='Validation RMSE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.title('Validation RMSE Over Time')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(session_checkpoint_dir, f'plots_epoch_{epoch+1}.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# Check if CUDA is available, if not, check for MPS (Metal Performance Shaders for Apple Silicon), otherwise use CPU\n",
    "\n",
    "# load aligned_df from path\n",
    "file_path = \"aligned_df.pq\"\n",
    "data_set = pd.read_parquet(file_path)\n",
    "\n",
    "\n",
    "# Define the compute_importances function\n",
    "def compute_importances(model, input_sequence):\n",
    "    # Initialize IntegratedGradients with the model\n",
    "    ig = IntegratedGradients(model)\n",
    "    \n",
    "    # Ensure the input sequence tensor is in float32 and add a batch dimension\n",
    "    input_tensor = torch.tensor(input_sequence, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Compute attributions using Integrated Gradients\n",
    "    attributions, delta = ig.attribute(input_tensor, return_convergence_delta=True)\n",
    "    \n",
    "    # Ensure the returned attributions are in float32\n",
    "    attributions = attributions.float()\n",
    "    \n",
    "    # Return the attributions and delta as numpy arrays\n",
    "    return attributions.detach().numpy(), delta.detach().numpy()\n",
    "\n",
    "# Dictionary to store aggregated importances for each experiment\n",
    "def layer_importances(model, data_set, features, sequence_length):\n",
    "    experiment_importances = {}\n",
    "\n",
    "    for exp_no, group in data_set.groupby('exp_no'):\n",
    "        # Initialize a list to store importances for all sequences in this experiment\n",
    "        all_importances = []\n",
    "\n",
    "        # Compute importances for each sequence within this experiment\n",
    "        for i in range(len(group) - sequence_length):\n",
    "            sequence = group[features].values[i:(i + sequence_length)]\n",
    "            # Assuming sequence is in the correct shape for the model\n",
    "            importances = compute_importances(model, sequence)\n",
    "            all_importances.append(importances)\n",
    "\n",
    "        # Aggregate the importances across all sequences for this experiment\n",
    "        # Here, we're taking the mean, but you could also sum them or use another method\n",
    "        aggregated_importances = np.mean(all_importances, axis=0)\n",
    "\n",
    "        # Store the aggregated importances in the dictionary\n",
    "        experiment_importances[exp_no] = aggregated_importances\n",
    "\n",
    "    return experiment_importances\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    \n",
    "    file_path = \"aligned_df.pq\"\n",
    "    data_set = pd.read_parquet(file_path)\n",
    "\n",
    "    features = ['timestamp_bin', 'A1_Resistance', 'A1_Resistance_diff']\n",
    "    target_column = 'resistance_ratio'\n",
    "    sequence_length = 100\n",
    "    step_size = 1\n",
    "    sequences_np, targets_np = sequence_generate(sequence_length, data_set, features, target_column, step_size)\n",
    "\n",
    "    sequences_train, sequences_test, targets_train, targets_test = split_data(sequences_np, targets_np)\n",
    "    sequences_train_scaled, sequences_test_scaled = scale_sequences(sequences_train, sequences_test)\n",
    "\n",
    "    batch_size = 32\n",
    "    train_sequences_tensor, test_sequences_tensor, train_targets_tensor, test_targets_tensor = convert_to_tensors(\n",
    "        sequences_train_scaled, sequences_test_scaled, targets_train, targets_test)\n",
    "    train_loader, test_loader = create_dataloaders(\n",
    "        train_sequences_tensor, train_targets_tensor, test_sequences_tensor, test_targets_tensor, batch_size=batch_size)\n",
    "\n",
    "    input_size = len(features)\n",
    "    hidden_size = 32\n",
    "    num_layers = 2\n",
    "    num_classes = 1\n",
    "    model = RNN(input_size, hidden_size, num_layers, num_classes)\n",
    "    learning_rate = 0.0001\n",
    "    criterion, optimizer = setup_training(model, learning_rate=learning_rate, device=device)\n",
    "\n",
    "    num_epochs = 10\n",
    "    checkpoint_dir = './checkpoints'\n",
    "    ensure_dir(checkpoint_dir)\n",
    "    session_timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    session_checkpoint_dir = os.path.join(checkpoint_dir, f'training_session_{session_timestamp}')\n",
    "    ensure_dir(session_checkpoint_dir)\n",
    "    run_directory = setup_directory(session_checkpoint_dir, f'importance_{session_timestamp}')\n",
    "\n",
    "    training_losses = []\n",
    "    validation_rmses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        torch.backends.cudnn.enabled = True\n",
    "        avg_training_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        training_losses.append(avg_training_loss)\n",
    "\n",
    "        avg_val_loss = validate(model, test_loader, criterion, device)\n",
    "        val_rmse = math.sqrt(avg_val_loss)\n",
    "        validation_rmses.append(val_rmse)\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Training Loss: {avg_training_loss}, Validation RMSE: {val_rmse}')\n",
    "\n",
    "        save_checkpoint(epoch, model, optimizer, avg_training_loss, val_rmse, session_checkpoint_dir)\n",
    "\n",
    "        # calculated feature importances\n",
    "        torch.backends.cudnn.enabled = False\n",
    "        spec_exp_imp(data_set, features, model, device, run_directory, sequence_length, specific_exp_no= 0, epoch_no=epoch)\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            plot_metrics(training_losses, validation_rmses, epoch, session_checkpoint_dir)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
