{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The kedro.ipython extension is already loaded. To reload it, use:\n",
      "  %reload_ext kedro.ipython\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05/16/24 12:49:44] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Loading data from <span style=\"color: #ff8700; text-decoration-color: #ff8700\">s3_conc_aligned_df</span> <span style=\"font-weight: bold\">(</span>ParquetDataset<span style=\"font-weight: bold\">)</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>           <a href=\"file:///Users/gavinlou/.pyenv/versions/3.10.13/envs/kedro/lib/python3.10/site-packages/kedro/io/data_catalog.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">data_catalog.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/gavinlou/.pyenv/versions/3.10.13/envs/kedro/lib/python3.10/site-packages/kedro/io/data_catalog.py#483\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">483</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[05/16/24 12:49:44]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading data from \u001b[38;5;208ms3_conc_aligned_df\u001b[0m \u001b[1m(\u001b[0mParquetDataset\u001b[1m)\u001b[0m\u001b[33m...\u001b[0m           \u001b]8;id=113790;file:///Users/gavinlou/.pyenv/versions/3.10.13/envs/kedro/lib/python3.10/site-packages/kedro/io/data_catalog.py\u001b\\\u001b[2mdata_catalog.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=627506;file:///Users/gavinlou/.pyenv/versions/3.10.13/envs/kedro/lib/python3.10/site-packages/kedro/io/data_catalog.py#483\u001b\\\u001b[2m483\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext kedro.ipython\n",
    "df = catalog.load(\"s3_conc_aligned_df\")\n",
    "# df = catalog.load(\"s3_cluster_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exp_no</th>\n",
       "      <th>timestamp_bin</th>\n",
       "      <th>A1_Resistance</th>\n",
       "      <th>A1_Resistance_diff</th>\n",
       "      <th>A1_Resistance_norm</th>\n",
       "      <th>A1_Sensor</th>\n",
       "      <th>A1_Sensor_diff</th>\n",
       "      <th>A1_Sensor_norm</th>\n",
       "      <th>SHT40_Humidity</th>\n",
       "      <th>SHT40_temp</th>\n",
       "      <th>index</th>\n",
       "      <th>resistance_ratio</th>\n",
       "      <th>ace_conc</th>\n",
       "      <th>expo_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>720650.750</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4518.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>42.835</td>\n",
       "      <td>29.430</td>\n",
       "      <td>4481.5</td>\n",
       "      <td>1.768473</td>\n",
       "      <td>3.033000e-07</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>720361.875</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.999599</td>\n",
       "      <td>4519.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000332</td>\n",
       "      <td>42.840</td>\n",
       "      <td>29.435</td>\n",
       "      <td>4483.5</td>\n",
       "      <td>1.768473</td>\n",
       "      <td>3.033000e-07</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>719688.470</td>\n",
       "      <td>-673.405</td>\n",
       "      <td>0.998665</td>\n",
       "      <td>4523.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.001107</td>\n",
       "      <td>42.845</td>\n",
       "      <td>29.445</td>\n",
       "      <td>4485.5</td>\n",
       "      <td>1.768473</td>\n",
       "      <td>3.033000e-07</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>719015.940</td>\n",
       "      <td>-1634.810</td>\n",
       "      <td>0.997731</td>\n",
       "      <td>4526.5</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1.001881</td>\n",
       "      <td>42.830</td>\n",
       "      <td>29.435</td>\n",
       "      <td>4487.5</td>\n",
       "      <td>1.768473</td>\n",
       "      <td>3.033000e-07</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>718727.905</td>\n",
       "      <td>-1345.095</td>\n",
       "      <td>0.997332</td>\n",
       "      <td>4528.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.002213</td>\n",
       "      <td>42.845</td>\n",
       "      <td>29.440</td>\n",
       "      <td>4489.5</td>\n",
       "      <td>1.768473</td>\n",
       "      <td>3.033000e-07</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "\n",
       "   exp_no  timestamp_bin  A1_Resistance  A1_Resistance_diff  \\\n",
       "\u001b[1;36m0\u001b[0m       \u001b[1;36m0\u001b[0m            \u001b[1;36m0.0\u001b[0m     \u001b[1;36m720650.750\u001b[0m               \u001b[1;36m0.000\u001b[0m   \n",
       "\u001b[1;36m1\u001b[0m       \u001b[1;36m0\u001b[0m            \u001b[1;36m1.0\u001b[0m     \u001b[1;36m720361.875\u001b[0m               \u001b[1;36m0.000\u001b[0m   \n",
       "\u001b[1;36m2\u001b[0m       \u001b[1;36m0\u001b[0m            \u001b[1;36m2.0\u001b[0m     \u001b[1;36m719688.470\u001b[0m            \u001b[1;36m-673.405\u001b[0m   \n",
       "\u001b[1;36m3\u001b[0m       \u001b[1;36m0\u001b[0m            \u001b[1;36m3.0\u001b[0m     \u001b[1;36m719015.940\u001b[0m           \u001b[1;36m-1634.810\u001b[0m   \n",
       "\u001b[1;36m4\u001b[0m       \u001b[1;36m0\u001b[0m            \u001b[1;36m4.0\u001b[0m     \u001b[1;36m718727.905\u001b[0m           \u001b[1;36m-1345.095\u001b[0m   \n",
       "\n",
       "   A1_Resistance_norm  A1_Sensor  A1_Sensor_diff  A1_Sensor_norm  \\\n",
       "\u001b[1;36m0\u001b[0m            \u001b[1;36m1.000000\u001b[0m     \u001b[1;36m4518.0\u001b[0m             \u001b[1;36m0.0\u001b[0m        \u001b[1;36m1.000000\u001b[0m   \n",
       "\u001b[1;36m1\u001b[0m            \u001b[1;36m0.999599\u001b[0m     \u001b[1;36m4519.5\u001b[0m             \u001b[1;36m0.0\u001b[0m        \u001b[1;36m1.000332\u001b[0m   \n",
       "\u001b[1;36m2\u001b[0m            \u001b[1;36m0.998665\u001b[0m     \u001b[1;36m4523.0\u001b[0m             \u001b[1;36m3.5\u001b[0m        \u001b[1;36m1.001107\u001b[0m   \n",
       "\u001b[1;36m3\u001b[0m            \u001b[1;36m0.997731\u001b[0m     \u001b[1;36m4526.5\u001b[0m             \u001b[1;36m8.5\u001b[0m        \u001b[1;36m1.001881\u001b[0m   \n",
       "\u001b[1;36m4\u001b[0m            \u001b[1;36m0.997332\u001b[0m     \u001b[1;36m4528.0\u001b[0m             \u001b[1;36m7.0\u001b[0m        \u001b[1;36m1.002213\u001b[0m   \n",
       "\n",
       "   SHT40_Humidity  SHT40_temp   index  resistance_ratio      ace_conc  \\\n",
       "\u001b[1;36m0\u001b[0m          \u001b[1;36m42.835\u001b[0m      \u001b[1;36m29.430\u001b[0m  \u001b[1;36m4481.5\u001b[0m          \u001b[1;36m1.768473\u001b[0m  \u001b[1;36m3.033000e-07\u001b[0m   \n",
       "\u001b[1;36m1\u001b[0m          \u001b[1;36m42.840\u001b[0m      \u001b[1;36m29.435\u001b[0m  \u001b[1;36m4483.5\u001b[0m          \u001b[1;36m1.768473\u001b[0m  \u001b[1;36m3.033000e-07\u001b[0m   \n",
       "\u001b[1;36m2\u001b[0m          \u001b[1;36m42.845\u001b[0m      \u001b[1;36m29.445\u001b[0m  \u001b[1;36m4485.5\u001b[0m          \u001b[1;36m1.768473\u001b[0m  \u001b[1;36m3.033000e-07\u001b[0m   \n",
       "\u001b[1;36m3\u001b[0m          \u001b[1;36m42.830\u001b[0m      \u001b[1;36m29.435\u001b[0m  \u001b[1;36m4487.5\u001b[0m          \u001b[1;36m1.768473\u001b[0m  \u001b[1;36m3.033000e-07\u001b[0m   \n",
       "\u001b[1;36m4\u001b[0m          \u001b[1;36m42.845\u001b[0m      \u001b[1;36m29.440\u001b[0m  \u001b[1;36m4489.5\u001b[0m          \u001b[1;36m1.768473\u001b[0m  \u001b[1;36m3.033000e-07\u001b[0m   \n",
       "\n",
       "   expo_time  \n",
       "\u001b[1;36m0\u001b[0m        \u001b[1;36m3.0\u001b[0m  \n",
       "\u001b[1;36m1\u001b[0m        \u001b[1;36m3.0\u001b[0m  \n",
       "\u001b[1;36m2\u001b[0m        \u001b[1;36m3.0\u001b[0m  \n",
       "\u001b[1;36m3\u001b[0m        \u001b[1;36m3.0\u001b[0m  \n",
       "\u001b[1;36m4\u001b[0m        \u001b[1;36m3.0\u001b[0m  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "\n",
    "\n",
    "# Check for missing values and handle them if necessary\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "# Define the features and target\n",
    "# features = ['timestamp_bin', 'A1_Resistance', 'cluster','distance_to_centroid_0','distance_to_centroid_1','distance_to_centroid_2']\n",
    "features = ['timestamp_bin', 'A1_Resistance']\n",
    "target = 'resistance_ratio'\n",
    "\n",
    "# Extract the feature matrix and target vector\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "groups = df['exp_no']\n",
    "\n",
    "# Initialize the GroupShuffleSplit object for training/validation split\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split into training and temporary (validation + testing) sets\n",
    "train_idx, temp_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "# Create DataFrames for training and temporary sets\n",
    "train_df = df.iloc[train_idx]\n",
    "temp_df = df.iloc[temp_idx]\n",
    "\n",
    "# Split the temporary set into validation and testing sets\n",
    "gss_temp = GroupShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "val_idx, test_idx = next(gss_temp.split(temp_df[features], temp_df[target], groups=temp_df['exp_no']))\n",
    "\n",
    "# Create DataFrames for validation and testing sets\n",
    "val_df = temp_df.iloc[val_idx]\n",
    "test_df = temp_df.iloc[test_idx]\n",
    "\n",
    "# Extract features and targets for each set\n",
    "X_train = train_df[features]\n",
    "y_train = train_df[target]\n",
    "X_val = val_df[features]\n",
    "y_val = val_df[target]\n",
    "X_test = test_df[features]\n",
    "y_test = test_df[target]\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import torch\n",
    "\n",
    "# Check for missing values and handle them if necessary\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "# Define the features and target\n",
    "initial_features = ['timestamp_bin', 'A1_Resistance']\n",
    "target = 'resistance_ratio'\n",
    "\n",
    "# Extract the feature matrix and target vector\n",
    "X = df[initial_features]\n",
    "y = df[target]\n",
    "groups = df['exp_no']\n",
    "\n",
    "# Initialize the GroupShuffleSplit object for training/validation split\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split into training and temporary (validation + testing) sets\n",
    "train_idx, temp_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "# Create DataFrames for training and temporary sets\n",
    "train_df = df.iloc[train_idx]\n",
    "temp_df = df.iloc[temp_idx]\n",
    "\n",
    "# Split the temporary set into validation and testing sets\n",
    "gss_temp = GroupShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "val_idx, test_idx = next(gss_temp.split(temp_df[initial_features], temp_df[target], groups=temp_df['exp_no']))\n",
    "\n",
    "# Create DataFrames for validation and testing sets\n",
    "val_df = temp_df.iloc[val_idx]\n",
    "test_df = temp_df.iloc[test_idx]\n",
    "\n",
    "# Extract features and targets for each set\n",
    "X_train = train_df[initial_features].copy()\n",
    "y_train = train_df[target]\n",
    "X_val = val_df[initial_features].copy()\n",
    "y_val = val_df[target]\n",
    "X_test = test_df[initial_features].copy()\n",
    "y_test = test_df[target]\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Apply K-Means clustering on the training data\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "X_train['cluster'] = kmeans.fit_predict(X_train_scaled)\n",
    "X_train_distances = kmeans.transform(X_train_scaled)\n",
    "for i in range(X_train_distances.shape[1]):\n",
    "    X_train[f'distance_to_centroid_{i}'] = X_train_distances[:, i]\n",
    "\n",
    "# Apply the same transformation to the validation and test data\n",
    "X_val['cluster'] = kmeans.predict(X_val_scaled)\n",
    "X_val_distances = kmeans.transform(X_val_scaled)\n",
    "for i in range(X_val_distances.shape[1]):\n",
    "    X_val[f'distance_to_centroid_{i}'] = X_val_distances[:, i]\n",
    "\n",
    "X_test['cluster'] = kmeans.predict(X_test_scaled)\n",
    "X_test_distances = kmeans.transform(X_test_scaled)\n",
    "for i in range(X_test_distances.shape[1]):\n",
    "    X_test[f'distance_to_centroid_{i}'] = X_test_distances[:, i]\n",
    "\n",
    "# Update the features list to include the new cluster features\n",
    "features = initial_features + ['cluster'] + [f'distance_to_centroid_{i}' for i in range(X_train_distances.shape[1])]\n",
    "\n",
    "# Extract features and targets for each set again\n",
    "X_train = X_train[features]\n",
    "X_val = X_val[features]\n",
    "X_test = X_test[features]\n",
    "\n",
    "# Normalize the features again after adding cluster information\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "\n",
    "class GroupDataset(Dataset):\n",
    "    def __init__(self, X, y, groups):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.groups = groups\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx], self.groups[idx]\n",
    "\n",
    "class GroupBatchSampler(Sampler):\n",
    "    def __init__(self, groups, batch_size):\n",
    "        self.groups = groups\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Create a dictionary mapping group labels to indices\n",
    "        self.group_to_indices = {}\n",
    "        for idx, group in enumerate(groups):\n",
    "            if group not in self.group_to_indices:\n",
    "                self.group_to_indices[group] = []\n",
    "            self.group_to_indices[group].append(idx)\n",
    "\n",
    "        # Create a list of group batches\n",
    "        self.group_batches = []\n",
    "        for group, indices in self.group_to_indices.items():\n",
    "            for i in range(0, len(indices), batch_size):\n",
    "                self.group_batches.append(indices[i:i + batch_size])\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Shuffle the group batches\n",
    "        np.random.shuffle(self.group_batches)\n",
    "        for batch in self.group_batches:\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.group_batches)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = GroupDataset(X_train_tensor, y_train_tensor, train_df['exp_no'].values)\n",
    "val_dataset = GroupDataset(X_val_tensor, y_val_tensor, val_df['exp_no'].values)\n",
    "test_dataset = GroupDataset(X_test_tensor, y_test_tensor, test_df['exp_no'].values)\n",
    "\n",
    "# Create batch samplers\n",
    "train_batch_sampler = GroupBatchSampler(train_dataset.groups, batch_size)\n",
    "val_batch_sampler = GroupBatchSampler(val_dataset.groups, batch_size)\n",
    "test_batch_sampler = GroupBatchSampler(test_dataset.groups, batch_size)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_sampler=train_batch_sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_sampler=val_batch_sampler)\n",
    "test_loader = DataLoader(test_dataset, batch_sampler=test_batch_sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Train Loss: 0.0428, Validation Loss: 0.0335\n",
      "Epoch [2/20], Train Loss: 0.0340, Validation Loss: 0.0333\n",
      "Epoch [3/20], Train Loss: 0.0339, Validation Loss: 0.0335\n",
      "Epoch [4/20], Train Loss: 0.0342, Validation Loss: 0.0339\n",
      "Epoch [5/20], Train Loss: 0.0337, Validation Loss: 0.0343\n",
      "Epoch [6/20], Train Loss: 0.0338, Validation Loss: 0.0339\n",
      "Epoch [7/20], Train Loss: 0.0338, Validation Loss: 0.0393\n",
      "Epoch [8/20], Train Loss: 0.0341, Validation Loss: 0.0319\n",
      "Epoch [9/20], Train Loss: 0.0336, Validation Loss: 0.0453\n",
      "Epoch [10/20], Train Loss: 0.0341, Validation Loss: 0.0333\n",
      "Epoch [11/20], Train Loss: 0.0339, Validation Loss: 0.0345\n",
      "Epoch [12/20], Train Loss: 0.0340, Validation Loss: 0.0328\n",
      "Epoch [13/20], Train Loss: 0.0339, Validation Loss: 0.0329\n",
      "Epoch [14/20], Train Loss: 0.0338, Validation Loss: 0.0325\n",
      "Epoch [15/20], Train Loss: 0.0338, Validation Loss: 0.0338\n",
      "Epoch [16/20], Train Loss: 0.0336, Validation Loss: 0.0339\n",
      "Epoch [17/20], Train Loss: 0.0337, Validation Loss: 0.0370\n",
      "Epoch [18/20], Train Loss: 0.0341, Validation Loss: 0.0351\n",
      "Epoch [19/20], Train Loss: 0.0340, Validation Loss: 0.0354\n",
      "Epoch [20/20], Train Loss: 0.0335, Validation Loss: 0.0353\n",
      "Test Loss: 0.0342\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class LinearNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LinearNN, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Define model, loss function, and optimizer\n",
    "input_dim = X_train.shape[1]\n",
    "model = LinearNN(input_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop with DataLoader\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for X_batch, y_batch, _ in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch.view(-1, 1))\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch, _ in val_loader:\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch.view(-1, 1))\n",
    "            val_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "# Testing the model\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch, _ in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch.view(-1, 1))\n",
    "        test_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kedro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
